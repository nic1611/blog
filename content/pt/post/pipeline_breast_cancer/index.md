---
date: "2021-11-23"
description: "Criando um pipeline de Machine Learning utilizando o Scikit-learn para prever o cancer de mama"
featured_image: https://images.unsplash.com/photo-1529101091764-c3526daf38fe?ixid=MXwxMjA3fDB8MHxzZWFyY2h8MzF8fG1hY2hpbmUlMjBsZWFybmluZ3xlbnwwfHwwfA%3D%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=500&q=60
tags:
- machine learning
- python
title: 'Criando um pipeline de Machine Learning utilizando o Scikit-learn'
---

# Criando um pipeline de Machine Learning utilizando o Scikit-learn


```python
from sklearn.datasets import load_breast_cancer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
```

### Vamos utilizar a base de dados breast cancer para essa demonstração


```python
cancer = load_breast_cancer()
X = cancer.data
Y = cancer.target
```

### Separamos as variaveis de dados e o nosso target entre teste e treino


```python
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, stratify=Y)
```

### Aqui, criamos nosso pipeline, definindo o método de normalização de dados StandadScaler, o PCA para redução de dimensionalidade e o KNN como nosso classificador


```python
pipeline = Pipeline([
        ('z-score', StandardScaler()),
        ('reduce_dim', PCA(n_components=3)),
        ('classify', KNeighborsClassifier(n_neighbors=1))])
```


```python
pipeline.fit(X_train, y_train)
```




    Pipeline(steps=[('z-score', StandardScaler()),
                    ('reduce_dim', PCA(n_components=3)),
                    ('classify', KNeighborsClassifier(n_neighbors=1))])




```python
from sklearn.metrics import accuracy_score

y_test_pred = pipeline.predict(X_test)
accuracy_score(y_test, y_test_pred)
```




    0.9035087719298246



### Exitem alguns hyper paramêtros que podem ser modificados para extrair um melhor resultado do modelo de classificação em questão.

### Utilizando o GridSearchCV em conjunto com o Pipeline, é possível automatizar o teste dos hyper parâmetros. O GridSearchCV, irá testar todas as variaveis definidas e guardar o resultado da acurácia de cada uma, uilizando a melhor como modelo classificador final.


```python
param_grid = {
    'reduce_dim__n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
    'classify__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
}

grid = GridSearchCV(pipeline, cv=2, n_jobs=1, param_grid=param_grid, scoring='accuracy')
```


```python
grid.fit(X_train, y_train)
```




    GridSearchCV(cv=2,
                 estimator=Pipeline(steps=[('z-score', StandardScaler()),
                                           ('reduce_dim', PCA(n_components=3)),
                                           ('classify',
                                            KNeighborsClassifier(n_neighbors=1))]),
                 n_jobs=1,
                 param_grid={'classify__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10,
                                                       11, 12, 13, 14, 15, 16, 17,
                                                       18],
                             'reduce_dim__n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9,
                                                          10, 11, 12, 13, 14, 15,
                                                          16, 17, 18, 19, 20, 21,
                                                          22, 23, 24, 25, 26, 27,
                                                          28, 29]},
                 scoring='accuracy')




```python
grid.cv_results_['mean_test_score']
```




    array([0.86367764, 0.91206817, 0.92092704, 0.94505951, 0.94065422,
           0.92967965, 0.94286653, 0.94067355, 0.94066388, 0.94066388,
           0.94066388, 0.9384709 , 0.9384709 , 0.94066388, 0.93627792,
           0.93627792, 0.93627792, 0.93627792, 0.93627792, 0.93627792,
           0.93627792, 0.93627792, 0.93627792, 0.93627792, 0.9384709 ,
           0.9384709 , 0.9384709 , 0.9384709 , 0.9384709 , 0.89007072,
           0.9362586 , 0.94506917, 0.94943581, 0.95382178, 0.94943581,
           0.94941649, 0.95822707, 0.96042971, 0.9538411 , 0.94943581,
           0.95604374, 0.95604374, 0.95385076, 0.95164812, 0.9538411 ,
           0.9538411 , 0.9538411 , 0.9538411 , 0.9538411 , 0.9538411 ,
           0.9538411 , 0.9538411 , 0.9538411 , 0.9538411 , 0.9538411 ,
           0.9538411 , 0.9538411 , 0.9538411 , 0.89224438, 0.93405595,
           0.9516771 , 0.94944547, 0.95603408, 0.94943581, 0.95602442,
           0.95601476, 0.95381212, 0.95383144, 0.95383144, 0.95823673,
           0.96042971, 0.96262269, 0.96262269, 0.96262269, 0.96262269,
           0.96262269, 0.96042005, 0.96042005, 0.96042005, 0.96262269,
           0.96262269, 0.96262269, 0.96262269, 0.96262269, 0.96262269,
           0.96262269, 0.96262269, 0.90104529, 0.93404629, 0.94948412,
           0.96044903, 0.95824639, 0.95823673, 0.95603408, 0.96263235,
           0.96703764, 0.96263235, 0.95603408, 0.95823673, 0.96042971,
           0.96263235, 0.96042971, 0.96042971, 0.96042971, 0.96042971,
           0.96042971, 0.96042971, 0.96042971, 0.96042971, 0.96042971,
           0.96042971, 0.96042971, 0.96042971, 0.96042971, 0.96042971,
           0.96042971, 0.90983654, 0.93624894, 0.95386042, 0.96043937,
           0.96042971, 0.96702798, 0.96043937, 0.96484466, 0.96483499,
           0.95603408, 0.95603408, 0.95604374, 0.96263235, 0.95823673,
           0.96263235, 0.96263235, 0.96263235, 0.96043937, 0.96263235,
           0.96263235, 0.96263235, 0.96263235, 0.96263235, 0.96043937,
           0.96263235, 0.96263235, 0.96263235, 0.96263235, 0.96263235,
           0.92523572, 0.93624894, 0.9560534 , 0.95163846, 0.95824639,
           0.96043937, 0.9560534 , 0.96266133, 0.96044903, 0.96043937,
           0.95823673, 0.96263235, 0.96042971, 0.95823673, 0.95823673,
           0.95823673, 0.95823673, 0.96043937, 0.96043937, 0.96043937,
           0.95823673, 0.95823673, 0.95823673, 0.95823673, 0.95823673,
           0.95823673, 0.95823673, 0.95823673, 0.95823673, 0.91643481,
           0.93403663, 0.94724283, 0.96042971, 0.96043937, 0.96482533,
           0.96483499, 0.96484466, 0.96044903, 0.95823673, 0.96043937,
           0.96262269, 0.96701832, 0.96701832, 0.96481567, 0.96481567,
           0.96261303, 0.96261303, 0.96261303, 0.96261303, 0.96261303,
           0.96261303, 0.96261303, 0.96261303, 0.96261303, 0.96261303,
           0.96261303, 0.96261303, 0.96261303, 0.91645413, 0.93846124,
           0.94944547, 0.96042971, 0.95823673, 0.96042971, 0.96044903,
           0.95825605, 0.9560534 , 0.95385076, 0.96044903, 0.96043937,
           0.95385076, 0.9560534 , 0.95604374, 0.95604374, 0.95604374,
           0.95604374, 0.95604374, 0.95604374, 0.95604374, 0.95604374,
           0.95385076, 0.95385076, 0.95385076, 0.95385076, 0.95604374,
           0.95604374, 0.95604374, 0.9208401 , 0.93404629, 0.94943581,
           0.95602442, 0.96262269, 0.96042971, 0.96264201, 0.96264201,
           0.96264201, 0.96483499, 0.96483499, 0.96263235, 0.95824639,
           0.96263235, 0.96043937, 0.96043937, 0.96043937, 0.96043937,
           0.96043937, 0.96043937, 0.96043937, 0.96043937, 0.96043937,
           0.96043937, 0.96043937, 0.96043937, 0.96043937, 0.96043937,
           0.96043937, 0.91426115, 0.93845158, 0.94725249, 0.95382178,
           0.96263235, 0.95604374, 0.9560534 , 0.96045869, 0.95604374,
           0.95824639, 0.96044903, 0.95824639, 0.95826571, 0.96044903,
           0.96044903, 0.95824639, 0.95824639, 0.95824639, 0.95824639,
           0.95824639, 0.95824639, 0.95824639, 0.95824639, 0.95824639,
           0.95824639, 0.95824639, 0.95824639, 0.95824639, 0.95824639,
           0.92304274, 0.93845158, 0.95163846, 0.95602442, 0.95602442,
           0.95823673, 0.95823673, 0.96043937, 0.96043937, 0.95824639,
           0.95604374, 0.95603408, 0.9560534 , 0.95824639, 0.95824639,
           0.95824639, 0.95824639, 0.95824639, 0.95604374, 0.95824639,
           0.95604374, 0.95604374, 0.95604374, 0.95604374, 0.95824639,
           0.95824639, 0.95824639, 0.95824639, 0.95824639, 0.92304274,
           0.93403663, 0.94286653, 0.95822707, 0.95822707, 0.9560534 ,
           0.95165778, 0.95825605, 0.9560534 , 0.9560534 , 0.95164812,
           0.95385076, 0.95824639, 0.95823673, 0.95823673, 0.95603408,
           0.95603408, 0.95603408, 0.95603408, 0.95603408, 0.95603408,
           0.9538411 , 0.9538411 , 0.9538411 , 0.9538411 , 0.95164812,
           0.9538411 , 0.95164812, 0.9538411 , 0.9208401 , 0.93185331,
           0.94065422, 0.9582174 , 0.96041039, 0.95603408, 0.95385076,
           0.9560534 , 0.96043937, 0.95824639, 0.95604374, 0.95604374,
           0.9560534 , 0.95823673, 0.9538411 , 0.9538411 , 0.9538411 ,
           0.9538411 , 0.9538411 , 0.9538411 , 0.9538411 , 0.9538411 ,
           0.9538411 , 0.9538411 , 0.9538411 , 0.9538411 , 0.9538411 ,
           0.9538411 , 0.9538411 , 0.91645413, 0.93624894, 0.94285687,
           0.95822707, 0.95163846, 0.95165778, 0.95386042, 0.95386042,
           0.9560534 , 0.9560534 , 0.95385076, 0.95385076, 0.95385076,
           0.95604374, 0.95604374, 0.95604374, 0.95604374, 0.95604374,
           0.9538411 , 0.95604374, 0.9538411 , 0.9538411 , 0.9538411 ,
           0.95163846, 0.95163846, 0.95163846, 0.95163846, 0.95163846,
           0.95163846, 0.9208401 , 0.92744803, 0.94285687, 0.95822707,
           0.9516288 , 0.95385076, 0.95165778, 0.9560534 , 0.96043937,
           0.95824639, 0.95604374, 0.95604374, 0.95824639, 0.95824639,
           0.95824639, 0.95824639, 0.95824639, 0.95824639, 0.95824639,
           0.95824639, 0.95824639, 0.95824639, 0.95824639, 0.95824639,
           0.95824639, 0.95824639, 0.95824639, 0.95824639, 0.95824639,
           0.91426115, 0.92965067, 0.94505951, 0.95383144, 0.95603408,
           0.95165778, 0.95165778, 0.95164812, 0.95823673, 0.95385076,
           0.95164812, 0.94945514, 0.94505951, 0.94945514, 0.95164812,
           0.95164812, 0.94945514, 0.94945514, 0.95164812, 0.95164812,
           0.94945514, 0.95164812, 0.95164812, 0.95164812, 0.95164812,
           0.95164812, 0.95164812, 0.95164812, 0.95164812, 0.91646379,
           0.92744803, 0.94725249, 0.95383144, 0.95603408, 0.94945514,
           0.95385076, 0.95385076, 0.95604374, 0.95385076, 0.95385076,
           0.95385076, 0.9538411 , 0.9538411 , 0.95604374, 0.95604374,
           0.95385076, 0.95385076, 0.95385076, 0.95385076, 0.95385076,
           0.95385076, 0.95385076, 0.95385076, 0.95164812, 0.95164812,
           0.95164812, 0.95164812, 0.95164812])



### Aqui, podemos ver quais hyper parâmetros são a melhor escolha para um modelo de classificação com melhor resultado


```python
print(grid.best_score_)
print(grid.best_params_)
```

    0.967037638148234
    {'classify__n_neighbors': 5, 'reduce_dim__n_components': 9}
    


```python
clf = grid.best_estimator_
```


```python
y_test_pred = clf.predict(X_test)
accuracy_score(y_test, y_test_pred)
```




    0.9736842105263158


